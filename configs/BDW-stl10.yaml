exp:
    benchmark: 'semi-supervised-rotation-prediction-stl10'
    name: 'meta-transductive-beta'
    seed: 1
    mode: 'default'
device: 'cuda:0'
trainer:
    algorithm: 'meta-gradient'
    num_epochs: 35
data:
    datasets:
        - 'stl10'
    test_set: 'stl10'
    split_mode: 'train_val_test'
    channels: 3
    resize: 96
    cropsize: 84
    h_flips: True
    training:
        mode: 'self-supervised'
        task: 'rotation-prediction'
        num_classes: 4
        batch_size: 32
        normalise: False
    validation:
        mode: 'supervised'
        task: 'prototypical-networks'
        num_classes_per_batch: 10
        num_data_per_class: 10
        num_queries: 10
        normalise: False
    test:
        mode: 'supervised'
        task: 'logistic-regression'
        num_classes: 10
        batch_size: 256
        logistic_regression:
            max_iter: 800
            normalise: False
        normalise: False
model:
    arch: 'resnet18'
    optimizer: 'sgd'
    lr: 0.1
    lr_gamma: 0.1
    lr_milestones:
        - 15
        - 25
    momentum: 0.9
    dampening: 0.
    nesterov: False
    wd: 0.0001
    new_weights: False
    fixed: False
    feature_layer: -1
weights:
    type: 'transductive'
    arch: 'beta-weights'
    optimizer: 'sgd'
    lr: 1
    lr_gamma: 1
    momentum: 0.9
    inner_model_lr: 0.01
    normalise: False
    lr_schedule: True
    max_grad_norm: 10
    clamp_at_zero: False
    clamp_at_one: False
    steps: 1
    baseline: False
    pruning:
        mode: 'cdf'
        epsilon: 0.1
        delta: 0.5
finetuning:
    batch_size: 64
    load_model: True
    episodes:
        - {epochs: 200, optimizer: 'adam', lr: 1e-4, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 1e-4, patience: 50}
        - {epochs: 200, optimizer: 'adam', lr: 1e-5, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 1e-4, patience: 50}
        - {epochs: 200, optimizer: 'adam', lr: 1e-4, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 0,    patience: 50}
        - {epochs: 200, optimizer: 'adam', lr: 1e-5, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 0,    patience: 50}
        - {epochs: 200, optimizer: 'adam', lr: 1e-4, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 1e-4, patience: 5 }
        - {epochs: 200, optimizer: 'adam', lr: 1e-5, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 1e-4, patience: 5 }
        - {epochs: 200, optimizer: 'adam', lr: 1e-4, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 0,    patience: 5 }
        - {epochs: 200, optimizer: 'adam', lr: 1e-5, lr_scheduler: 'reduce_lr_on_plateau', mode: 'min', wd: 0,    patience: 5 }
